
Data Processing and Aggregation with Apache Spark and MongoDB

The workflow includes:
- Loading CSV data into Spark DataFrame
- Cleaning and filtering the dataset
- Performing multiple aggregations
- Writing aggregated results into MongoDB collections
- Running MongoDB queries for verification

The MongoDB service will be available on:
	•	Port: 27018 (mapped to container's 27017)
	•	Username: root

Run Spark in Docker
docker run --rm \
  --network host \
  -v $(pwd):/app \
  bitnami/spark:latest \
  spark-submit \
    --packages org.mongodb.spark:mongo-spark-connector_2.12:10.2.1 \
    /app/spark_job.py


MongoDB Collections
product_metrics: Review count and average rating per product
customer_review_counts: Number of verified reviews per customer
product_monthly_reviews: Monthly aggregated review counts per product


Screenshots
